{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CRF\n",
    "- CRF 模型的参数是什么\n",
    "- 如何优化参数\n",
    "- 如何使用参数来做预测\n",
    "\n",
    "## 投掷骰子的转移矩阵(transition matrix)\n",
    "![image](https://cdn-images-1.medium.com/max/1000/1*y--8BwbkERsoTSHhKJVgMw.png)\n",
    "\n",
    "## 参数估计\n",
    "![image](https://cdn-images-1.medium.com/max/1600/1*wIDuc1ApkbU8bAcIRxauag.png)\n",
    "![image](https://cdn-images-1.medium.com/max/2400/1*-aGstuftCCBeDkSuo_GWeA.png)\n",
    "\n",
    "- $y_i$：第 i 步骰子的 label\n",
    "- $P(x_i|y_i)$：在确定了第 i 步骰子的 label 后，第 i 步出现值 x 的概率。例如，如果第 i 步是公平的筛子，则出现任何值的概率都是 1/6\n",
    "- $T(y_i|y_{i-1})$：根据骰子的 label 从转移矩阵中获得对应的 cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Inspired by http://pytorch.org/tutorials/beginner/nlp/advanced_tutorial.html\"\"\"\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from torch.optim import Adam\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class CRF(nn.Module):\n",
    "    \n",
    "    def __init__(self, n_dice, log_likelihood):\n",
    "        super(CRF, self).__init__()\n",
    "        \n",
    "        self.n_states = n_dice\n",
    "        self.transition = torch.nn.init.normal(nn.Parameter(torch.randn(n_dice, n_dice + 1)), -1, 0.1)\n",
    "        self.loglikelihood = log_likelihood\n",
    "    \n",
    "\n",
    "    def to_scalar(self, var):\n",
    "        return var.view(-1).data.tolist()[0]\n",
    "\n",
    "\n",
    "    def argmax(self, vec):\n",
    "        _, idx = torch.max(vec, 1)\n",
    "        return self.to_scalar(idx)\n",
    "        \n",
    "    # numerically stable log sum exp\n",
    "    # Source: http://pytorch.org/tutorials/beginner/nlp/advanced_tutorial.html\n",
    "    def log_sum_exp(self, vec):\n",
    "        max_score = vec[0, self.argmax(vec)]\n",
    "        max_score_broadcast = max_score.view(1, -1).expand(1, vec.size()[1])\n",
    "        return max_score + \\\n",
    "               torch.log(torch.sum(torch.exp(vec - max_score_broadcast)))\n",
    "    \n",
    "    \n",
    "    def _data_to_likelihood(self, rolls):\n",
    "        \"\"\"Converts a numpy array of rolls (integers) to log-likelihood.\n",
    "\n",
    "        Input is one [1, n_rolls]\n",
    "        \"\"\"\n",
    "        return Variable(torch.FloatTensor(self.loglikelihood[rolls]), requires_grad=False)\n",
    "        \n",
    "    \n",
    "    def _compute_likelihood_numerator(self, loglikelihoods, states):\n",
    "        \"\"\"Computes numerator of likelihood function for a given sequence.\n",
    "        \n",
    "        We'll iterate over the sequence of states and compute the sum \n",
    "        of the relevant transition cost with the log likelihood of the observed\n",
    "        roll. \n",
    "\n",
    "        Input:\n",
    "            loglikelihoods: torch Variable. Matrix of n_obs x n_states. \n",
    "                            i,j entry is loglikelihood of observing roll i given state j\n",
    "            states: sequence of labels\n",
    "        Output:\n",
    "            score: torch Variable. Score of assignment. \n",
    "        \"\"\"\n",
    "        prev_state = self.n_states\n",
    "        score = Variable(torch.Tensor([0]))\n",
    "        for index, state in enumerate(states):\n",
    "            score += self.transition[state, prev_state] + loglikelihoods[index, state]\n",
    "            prev_state = state\n",
    "        return score\n",
    "    \n",
    "    def _compute_likelihood_denominator(self, loglikelihoods):\n",
    "        \"\"\"Implements the forward pass of the forward-backward algorithm.\n",
    "        \n",
    "        We loop over all possible states efficiently using the recursive\n",
    "        relationship: alpha_t(j) = \\sum_i alpha_{t-1}(i) * L(x_t | y_t) * C(y_t | y{t-1} = i)\n",
    "\n",
    "        Input:\n",
    "            loglikelihoods: torch Variable. Same input as _compute_likelihood_numerator.\n",
    "                            This algorithm efficiently loops over all possible state sequences\n",
    "                            so no other imput is needed.\n",
    "        Output:\n",
    "            torch Variable. \n",
    "        \"\"\"\n",
    "        # Stores the current value of alpha at timestep t\n",
    "        prev_alpha = self.transition[:, self.n_states] + loglikelihoods[0].view(1, -1)\n",
    "\n",
    "        for roll in loglikelihoods[1:]:\n",
    "            alpha_t = []\n",
    "\n",
    "            # Loop over all possible states\n",
    "            for next_state in range(self.n_states):\n",
    "                \n",
    "                # Compute all possible costs of transitioning to next_state\n",
    "                feature_function = self.transition[next_state,:self.n_states].view(1, -1) +\\\n",
    "                                   roll[next_state].view(1, -1).expand(1, self.n_states)\n",
    "\n",
    "                alpha_t_next_state = prev_alpha + feature_function\n",
    "                alpha_t.append(self.log_sum_exp(alpha_t_next_state))\n",
    "            prev_alpha = torch.cat(alpha_t).view(1, -1)\n",
    "        return self.log_sum_exp(prev_alpha)\n",
    "    \n",
    "    def _viterbi_algorithm(self, loglikelihoods):\n",
    "        \"\"\"Implements Viterbi algorithm for finding most likely sequence of labels.\n",
    "        \n",
    "        Very similar to _compute_likelihood_denominator but now we take the maximum\n",
    "        over the previous states as opposed to the sum. \n",
    "\n",
    "        Input:\n",
    "            loglikelihoods: torch Variable. Same input as _compute_likelihood_denominator.\n",
    "\n",
    "        Output:\n",
    "            tuple. First entry is the most likely sequence of labels. Second is\n",
    "                   the loglikelihood of this sequence. \n",
    "        \"\"\"\n",
    "\n",
    "        argmaxes = []\n",
    "\n",
    "        # prev_delta will store the current score of the sequence for each state\n",
    "        prev_delta = self.transition[:, self.n_states].contiguous().view(1, -1) +\\\n",
    "                      loglikelihoods[0].view(1, -1)\n",
    "\n",
    "        for roll in loglikelihoods[1:]:\n",
    "            local_argmaxes = []\n",
    "            next_delta = []\n",
    "            for next_state in range(self.n_states):\n",
    "                feature_function = self.transition[next_state,:self.n_states].view(1, -1) +\\\n",
    "                                   roll.view(1, -1) +\\\n",
    "                                   prev_delta\n",
    "                most_likely_state = self.argmax(feature_function)\n",
    "                score = feature_function[0][most_likely_state]\n",
    "                next_delta.append(score)\n",
    "                local_argmaxes.append(most_likely_state)\n",
    "            prev_delta = torch.cat(next_delta).view(1, -1)\n",
    "            argmaxes.append(local_argmaxes)\n",
    "        \n",
    "        final_state = self.argmax(prev_delta)\n",
    "        final_score = prev_delta[0][final_state]\n",
    "        path_list = [final_state]\n",
    "\n",
    "        # Backtrack through the argmaxes to find most likely state\n",
    "        for states in reversed(argmaxes):\n",
    "            final_state = states[final_state]\n",
    "            path_list.append(final_state)\n",
    "        \n",
    "        return np.array(path_list), final_score\n",
    "        \n",
    "    def neg_log_likelihood(self, rolls, states):\n",
    "        \"\"\"Compute neg log-likelihood for a given sequence.\n",
    "        \n",
    "        Input: \n",
    "            rolls: numpy array, dim [1, n_rolls]. Integer 0-5 showing value on dice.\n",
    "            states: numpy array, dim [1, n_rolls]. Integer 0, 1. 0 if dice is fair.\n",
    "        \"\"\"\n",
    "        loglikelihoods = self._data_to_likelihood(rolls)\n",
    "        states = torch.LongTensor(states)\n",
    "        \n",
    "        sequence_loglik = self._compute_likelihood_numerator(loglikelihoods, states)\n",
    "        denominator = self._compute_likelihood_denominator(loglikelihoods)\n",
    "        return denominator - sequence_loglik\n",
    "               \n",
    "    \n",
    "    def forward(self, rolls):\n",
    "        loglikelihoods = self._data_to_likelihood(rolls)\n",
    "        return self._viterbi_algorithm(loglikelihoods)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from torch.optim import Adam\n",
    "\n",
    "def crf_train_loop(model, rolls, targets, n_epochs, learning_rate=0.01):\n",
    "    optimizer = Adam(model.parameters(), \n",
    "                     lr=learning_rate,\n",
    "                     weight_decay=1e-4)\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        batch_loss = []\n",
    "        N = rolls.shape[0]\n",
    "        model.zero_grad()\n",
    "        for index, (roll, labels) in enumerate(zip(rolls, targets)):\n",
    "            # Forward Pass\n",
    "            neg_log_likelihood = model.neg_log_likelihood(roll, labels)\n",
    "            batch_loss.append(neg_log_likelihood)\n",
    "            \n",
    "            if index % 50 == 0:\n",
    "                ll = torch.cat(batch_loss).mean()\n",
    "                ll.backward()\n",
    "                optimizer.step()\n",
    "                print(\"Epoch {}: Batch {}/{} loss is {:.4f}\".format(epoch, index//50,N//50,ll.data.numpy()[0]))\n",
    "                butiatch_loss = []\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/ipykernel_launcher.py:15: UserWarning: nn.init.normal is now deprecated in favor of nn.init.normal_.\n",
      "  from ipykernel import kernelapp as app\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> <ipython-input-1-ce8b5ae52ee1>(81)_compute_likelihood_denominator()\n",
      "-> prev_alpha = self.transition[:, self.n_states] + loglikelihoods[0].view(1, -1)\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "(Pdb)  ll\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 64  \t    def _compute_likelihood_denominator(self, loglikelihoods):\n",
      " 65  \t        \"\"\"Implements the forward pass of the forward-backward algorithm.\n",
      " 66  \t\n",
      " 67  \t        We loop over all possible states efficiently using the recursive\n",
      " 68  \t        relationship: alpha_t(j) = \\sum_i alpha_{t-1}(i) * L(x_t | y_t) * C(y_t | y{t-1} = i)\n",
      " 69  \t\n",
      " 70  \t        Input:\n",
      " 71  \t            loglikelihoods: torch Variable. Same input as _compute_likelihood_numerator.\n",
      " 72  \t                            This algorithm efficiently loops over all possible state sequences\n",
      " 73  \t                            so no other imput is needed.\n",
      " 74  \t        Output:\n",
      " 75  \t            torch Variable.\n",
      " 76  \t        \"\"\"\n",
      " 77  \t        import pdb\n",
      " 78  \t        pdb.set_trace()\n",
      " 79  \t\n",
      " 80  \t        # Stores the current value of alpha at timestep t\n",
      " 81  ->\t        prev_alpha = self.transition[:, self.n_states] + loglikelihoods[0].view(1, -1)\n",
      " 82  \t\n",
      " 83  \t        for roll in loglikelihoods[1:]:\n",
      " 84  \t            alpha_t = []\n",
      " 85  \t\n",
      " 86  \t            # Loop over all possible states\n",
      " 87  \t            for next_state in range(self.n_states):\n",
      " 88  \t\n",
      " 89  \t                # Compute all possible costs of transitioning to next_state\n",
      " 90  \t                feature_function = self.transition[next_state,:self.n_states].view(1, -1) +                                   roll[next_state].view(1, -1).expand(1, self.n_states)\n",
      " 91  \t\n",
      " 92  \t                alpha_t_next_state = prev_alpha + feature_function\n",
      " 93  \t                alpha_t.append(self.log_sum_exp(alpha_t_next_state))\n",
      " 94  \t            prev_alpha = torch.cat(alpha_t).view(1, -1)\n",
      " 95  \t        return self.log_sum_exp(prev_alpha)\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "(Pdb)  loglikelihoods\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-1.7918, -0.2231],\n",
      "        [-1.7918, -3.2189],\n",
      "        [-1.7918, -3.2189],\n",
      "        [-1.7918, -0.2231],\n",
      "        [-1.7918, -0.2231],\n",
      "        [-1.7918, -3.2189],\n",
      "        [-1.7918, -3.2189],\n",
      "        [-1.7918, -0.2231],\n",
      "        [-1.7918, -0.2231],\n",
      "        [-1.7918, -3.2189],\n",
      "        [-1.7918, -0.2231],\n",
      "        [-1.7918, -0.2231],\n",
      "        [-1.7918, -3.2189],\n",
      "        [-1.7918, -0.2231],\n",
      "        [-1.7918, -0.2231]])\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "(Pdb)  q\n"
     ]
    },
    {
     "ename": "BdbQuit",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mBdbQuit\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-1c2c4e058e4e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCRF\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_likelihood\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcrf_train_loop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrolls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.001\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"./checkpoint.hdf5\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-7-d75a6fa425c8>\u001b[0m in \u001b[0;36mcrf_train_loop\u001b[0;34m(model, rolls, targets, n_epochs, learning_rate)\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mroll\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrolls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m             \u001b[0;31m# Forward Pass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m             \u001b[0mneg_log_likelihood\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mneg_log_likelihood\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroll\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m             \u001b[0mbatch_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mneg_log_likelihood\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-1-ce8b5ae52ee1>\u001b[0m in \u001b[0;36mneg_log_likelihood\u001b[0;34m(self, rolls, states)\u001b[0m\n\u001b[1;32m    148\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m         \u001b[0msequence_loglik\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compute_likelihood_numerator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloglikelihoods\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m         \u001b[0mdenominator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compute_likelihood_denominator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloglikelihoods\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdenominator\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0msequence_loglik\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-1-ce8b5ae52ee1>\u001b[0m in \u001b[0;36m_compute_likelihood_denominator\u001b[0;34m(self, loglikelihoods)\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m         \u001b[0;31m# Stores the current value of alpha at timestep t\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 81\u001b[0;31m         \u001b[0mprev_alpha\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransition\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_states\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mloglikelihoods\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     82\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mroll\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mloglikelihoods\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-1-ce8b5ae52ee1>\u001b[0m in \u001b[0;36m_compute_likelihood_denominator\u001b[0;34m(self, loglikelihoods)\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m         \u001b[0;31m# Stores the current value of alpha at timestep t\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 81\u001b[0;31m         \u001b[0mprev_alpha\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransition\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_states\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mloglikelihoods\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     82\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mroll\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mloglikelihoods\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/Cellar/python/3.6.5_1/Frameworks/Python.framework/Versions/3.6/lib/python3.6/bdb.py\u001b[0m in \u001b[0;36mtrace_dispatch\u001b[0;34m(self, frame, event, arg)\u001b[0m\n\u001b[1;32m     49\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;31m# None\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mevent\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'line'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_line\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mevent\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'call'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/Cellar/python/3.6.5_1/Frameworks/Python.framework/Versions/3.6/lib/python3.6/bdb.py\u001b[0m in \u001b[0;36mdispatch_line\u001b[0;34m(self, frame)\u001b[0m\n\u001b[1;32m     68\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_here\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbreak_here\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muser_line\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mquitting\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mraise\u001b[0m \u001b[0mBdbQuit\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrace_dispatch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mBdbQuit\u001b[0m: "
     ]
    }
   ],
   "source": [
    "main"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 参考\n",
    "- https://pytorch.org/tutorials/beginner/nlp/advanced_tutorial.html\n",
    "- https://towardsdatascience.com/conditional-random-field-tutorial-in-pytorch-ca0d04499463\n",
    "- forward-backward algorithm: https://en.wikipedia.org/wiki/Forward%E2%80%93backward_algorithm\n",
    "- Viterbi algorithm: https://en.wikipedia.org/wiki/Viterbi_algorithm"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
