{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Softmax 函数的作用是输入一个 N 维的向量，将向量中的值映射到 $(0, 1)$ 的范围内，并且所有值的累加和为 1，相当于输出一个概率分布，可以将输出的值理解为输入向量属于对应分类的概率，因此常将 softmax 函数适合用在多任务分类中，放在网络的最后一层，作为输出层。\n",
    "\n",
    "## 公式推导\n",
    "\n",
    "原始公式： \n",
    "\n",
    "\\begin{align}\n",
    "S_j = \\frac{e^{a_j}}{\\sum_{k=1}^N e^{a_k}}\n",
    "\\qquad \\forall j \\in 1..N\\\n",
    "\\end{align}\n",
    "\n",
    "softmax 的“偏导数”：softmax 函数有多个输入/输出，所以求偏导时需要和指定是求哪一个输出对哪一个输入的偏导：\n",
    "\n",
    "\\begin{align}\n",
    "\\frac{\\partial S_i}{\\partial a_j}=\n",
    "\\frac{\\partial \\frac{e^{a_i}}{\\sum_{k=1}^{N}e^{a_k}}}{\\partial a_j}\n",
    "\\end{align}\n",
    "\n",
    "分数的求导数公式，对于 $f(x) = \\frac{g(x)}{h(x)}$，其导数为：\n",
    "\n",
    "\\begin{align}\n",
    "f'(x) = \\frac{g'(x)h(x) - h'(x)g(x)}{[h(x)]^2}\n",
    "\\end{align}\n",
    "\n",
    "对于 softmax 函数来说，这里的 $g(x), h(x)$ 分别为：\n",
    "\n",
    "\\begin{align*}\n",
    "g_i&=e^{a_i} \\\\\n",
    "h_i&=\\sum_{k=1}^{N}e^{a_k}\n",
    "\\end{align*}\n",
    "\n",
    "对于 $h_i$ 来说，$\\frac{\\partial h_i}{\\partial a_j}$ 永远等于 $e^{a_j}$；对于 $g_i$ 来说，只有当 $i=j$ 时 $\\frac{\\partial g_i}{\\partial a_j}$ 才等于 $e^{a_j}$，对于其他 i 值，$\\frac{\\partial g_i}{\\partial a_j}$ 均为 0。\n",
    "\n",
    "对于 $i==j$ 的情况，求偏导的推导过程如下，其中 $\\sum$ 代表 $\\sum_{k=1}^{N}e^{a_k}$：\n",
    "\n",
    "\\begin{align*}\n",
    "\\frac{\\partial \\frac{e^{a_i}}{\\sum_{k=1}^{N}e^{a_k}}}{\\partial a_j}&=\n",
    "\\frac{e^{a_i}\\Sigma-e^{a_j}e^{a_i}}{\\Sigma^2}\\\\\n",
    "&=\\frac{e^{a_i}}{\\Sigma}\\frac{\\Sigma - e^{a_j}}{\\Sigma}\\\\\n",
    "&=S_i(1-S_j)\n",
    "\\end{align*}\n",
    "\n",
    "对于 $i\\ne j$ 的情况，求偏导的推导过程如下：\n",
    "\n",
    "\\begin{align*}\n",
    "\\frac{\\partial \\frac{e^{a_i}}{\\sum_{k=1}^{N}e^{a_k}}}{\\partial a_j}&=\n",
    "\\frac{0-e^{a_j}e^{a_i}}{\\Sigma^2}\\\\\n",
    "&=-\\frac{e^{a_j}}{\\Sigma}\\frac{e^{a_i}}{\\Sigma}\\\\\n",
    "&=-S_j S_i\n",
    "\\end{align*}\n",
    "\n",
    "最终，softmax 求偏导的结果可以写成：\n",
    "\n",
    "\\begin{align*}\n",
    "\\frac{\\partial S_i}{\\partial a_j}=\\left\\{\n",
    "\\begin{matrix}\n",
    "S_i(1-S_j) & i=j\\\\\n",
    "-S_j S_i & i\\ne j\n",
    "\\end{matrix}\n",
    "\\right.\n",
    "\\end{align*}\n",
    "\n",
    "也可以写成：\n",
    "\n",
    "\\begin{align*}\n",
    "\\delta_{ij}=\\left\\{\\begin{matrix}\n",
    "1 & i=j\\\\\n",
    "0 & i\\ne j\n",
    "\\end{matrix}\\right. \\\\\n",
    "\\frac{\\partial S_i}{\\partial a_j} = S_i (\\delta_{ij}-S_j)\n",
    "\\end{align*}\n",
    "\n",
    "## 交叉熵损失函数(cross-entropy loss)\n",
    "\n",
    "Softmax 常和交叉熵损失函数一起使用，对于两个离散的概率分布 $p$ 和 $q$，交叉熵函数的定义如下：\n",
    "\n",
    "\\begin{align*}\n",
    "xent(p,q)=-\\sum_{k}p(k)log(q(k))\n",
    "\\end{align*}\n",
    "\n",
    "对于分类任务，假设有 $T$ 个分类，$k$ 就为从 $1$ 到 $T$。假设 $P$ 表示 softmax 的预测结果输出，$Y$ 表示以 one-hot 形式表示的正确分类结果，则分类任务的交叉熵损失函数为：\n",
    "\n",
    "\\begin{align*}\n",
    "xent(Y, P)=-\\sum_{k=1}^{T}Y(k)log(P(k))\n",
    "\\end{align*}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "参考\n",
    "- https://eli.thegreenplace.net/2016/the-softmax-function-and-its-derivative/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
